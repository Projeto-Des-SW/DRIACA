import os
import pickle
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


load_dotenv()
# --- Laborat√≥rio de Experimentos ---
# MODIFIQUE ESTAS VARI√ÅVEIS PARA TESTAR DIFERENTES CONFIGURA√á√ïES
# --------------------------------------------------------------------------
# Modelos de Embedding para testar:
# - "sentence-transformers/all-MiniLM-L6-v2" (ingl√™s/multil√≠ngue, r√°pido)
# - "neuralmind/bert-base-portuguese-cased-sts" (bom para portugu√™s)
# - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2" (mais robusto)
EMBED_MODEL_ID = os.getenv("EMBED_MODEL_ID") 

# Estrat√©gia de Chunking para testar:
CHUNK_SIZE = 1000 
CHUNK_OVERLAP = 200

# Nome do √≠ndice de sa√≠da (pode ser √∫til mudar se quiser manter v√°rias vers√µes)
FAISS_INDEX_PATH = "faiss_index"
# --------------------------------------------------------------------------


# --- Configura√ß√µes Gerais ---
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["HUGGINGFACE_HUB_DISABLE_SYMLINKS"] = "1"
INPUT_DOCS_FILE = "processed_docs.pkl"

def create_vectorstore():
    """
    Carrega documentos pr√©-processados e cria um Vector Store com
    as configura√ß√µes de chunking e embedding especificadas.
    """
    print("üöÄ Etapa 2: Iniciando a cria√ß√£o do Vector Store...")
    
    # 1. Carregar os documentos pr√©-processados do arquivo pickle
    print(f"üîÑ Carregando documentos de '{INPUT_DOCS_FILE}'...")
    try:
        with open(INPUT_DOCS_FILE, "rb") as f:
            processed_docs = pickle.load(f)
    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo '{INPUT_DOCS_FILE}' n√£o encontrado.")
        print("‚û°Ô∏è Por favor, execute 'python load_docs.py' primeiro.")
        return
    print(f"‚úÖ {len(processed_docs)} documentos carregados.")

    # 2. Aplicar a estrat√©gia de Chunking
    print(f"üìÑ Aplicando chunking: size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", ", ", " ", ""],
    )
    splits = text_splitter.split_documents(processed_docs)
    print(f"‚úÖ Documentos divididos em {len(splits)} chunks.")
    
    # 3. Criar os Embeddings e o Vector Store
    print(f"üß† Criando embeddings com o modelo: {EMBED_MODEL_ID}")
    embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)

    print(f"üíæ Criando e salvando o √≠ndice FAISS em '{FAISS_INDEX_PATH}'...")
    if not splits:
        print("‚ùå Nenhum chunk foi criado. Abortando a cria√ß√£o do √≠ndice.")
        return
        
    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)
    vectorstore.save_local(FAISS_INDEX_PATH)
    print(f"üéâ Etapa 2 conclu√≠da! Vector Store criado com sucesso.")

if __name__ == "__main__":
    create_vectorstore()